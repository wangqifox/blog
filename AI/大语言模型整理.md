---
title: å¤§è¯­è¨€æ¨¡å‹æ•´ç†
date: 2023/05/29 10:18:00
---

<!-- more -->

éšç€ChatGPTçš„ç«çˆ†ï¼Œå¤§è¯­è¨€æ¨¡å‹é¢†åŸŸåˆå¼€å§‹çƒ­é—¹èµ·æ¥ã€‚ä¸è¿‡GPT-3.5å’ŒGPT-4å¹¶æ²¡æœ‰å¼€æºï¼Œéšç€Metaå¼€æºäº†LLaMAæ¨¡å‹ï¼Œç›¸å…³çš„å¼€æºæ¨¡å‹é€æ¸å¼€å§‹èµ°è¿›äººä»¬è§†é‡ã€‚

![llm-1](media/llm-1.png)

# LLaMA

Metaâ€œæ³„éœ²â€çš„å¤§è¯­è¨€æ¨¡å‹ã€‚`magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&dn=LLaMA`



# llama cpp

ç”¨äºLLaMAæ¨¡å‹æ¨ç†çš„çº¯C++ç‰ˆæœ¬ï¼Œå¯ä»¥åœ¨CPUç¯å¢ƒä¸‹è¿è¡ŒLLaMAæ¨¡å‹ã€‚åŒ…æ‹¬LLaMAä»¥åŠAlpacaç­‰è¡ç”Ÿçš„æ¨¡å‹ã€‚

[`https://github.com/ggerganov/llama.cpp`](https://github.com/ggerganov/llama.cpp)



## ç¼–è¯‘

cloneä»£ç ï¼š

```Bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```

ä½¿ç”¨`make`æ¥ç¼–è¯‘ï¼š

```Bash
make
```

æˆ–è€…ä½¿ç”¨`cmake`æ¥ç¼–è¯‘ï¼š

```Bash
mkdir build
cd build
cmake ..
cmake --build . --config Release
```



## BLASç¼–è¯‘ä½¿ç”¨

ç¼–è¯‘æ—¶æ”¯æŒBLAS

### OpenBLAS

ä½¿ç”¨`make`ï¼š

```Bash
make LLAMA_OPENBLAS=1

```

ä½¿ç”¨`cmake`ï¼š

```Bash
mkdir build
cd build
cmake .. -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS
cmake --build . --config Release
```



### cuBLAS

cuBLASä½¿ç”¨CUDAæ¥åŠ é€Ÿç®—æ³•ã€‚

ä½¿ç”¨`make`ï¼š

```Bash
make LLAMA_CUBLAS=1

```

ä½¿ç”¨`cmake`ï¼š

```Bash
mkdir build
cd build
cmake .. -DLLAMA_CUBLAS=ON
cmake --build . --config Release
```

ä½¿ç”¨GPUï¼Œéœ€è¦åœ¨è¿è¡Œæ—¶åŠ ä¸Š`-ngl`å‚æ•°ï¼ŒæŒ‡å®šå¤šå°‘layerä¿å­˜åœ¨GPUä¸Š



## è¿è¡Œllamaæ¨¡å‹

å‡†å¤‡æ•°æ®å¹¶è¿è¡Œ

```Bash
# obtain the original LLaMA model weights and place them in ./models
ls ./models
65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model

# install Python dependencies
python3 -m pip install -r requirements.txt

# convert the 7B model to ggml FP16 format
python3 convert.py models/7B/

# quantize the model to 4-bits (using q4_0 method)
./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin q4_0

# run the inference
./main -m ./models/7B/ggml-model-q4_0.bin -n 128
```

æ¨ç†çš„å…¸å‹ç”¨æ³•ï¼š

```Bash
./main -m ./models/7B/ggml-model-q4_0.bin -p "Building a website can be done in 10 simple steps:" -n 512
```

# alpaca

Alpacaæ˜¯åœ¨LLaMAä¸Šè¿›è¡Œå¾®è°ƒçš„ã€‚è€Œä¸ºäº†è®­ç»ƒè¿™ä¸ªè¯­è¨€æ¨¡å‹ï¼Œç§‘å­¦å®¶ä»¬ä½¿ç”¨OpenAIçš„ â€œtext-davinci-003 â€œæ¨¡å‹ï¼Œç”Ÿæˆäº†52Ké«˜è´¨é‡çš„è‡ªæˆ‘æŒ‡å¯¼æ•°æ®ã€‚æœ‰äº†è¿™ä¸ªæ•°æ®é›†ï¼Œä»–ä»¬ä½¿ç”¨HuggingFaceçš„è®­ç»ƒæ¡†æ¶å¯¹LLaMAæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚

[`https://github.com/tatsu-lab/stanford_alpaca`](https://github.com/tatsu-lab/stanford_alpaca)



## è¿è¡Œ

å¯ä»¥ä½¿ç”¨llama.cppæ¥è¿è¡Œã€‚ä¸‹è½½ggmlçš„[Alpacaæ¨¡å‹](https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml/blob/main/ggml-alpaca-7b-q4.bin)ï¼Œæ”¾åˆ°`./models`ç›®å½•ã€‚æ¥ç€æ‰§è¡Œï¼š

```Bash
./examples/alpaca.sh
```





# alpaca-lora

åœ¨æ¶ˆè´¹çº§GPUä¸Šå¾®è°ƒâ€œåŸºäºLLaMAçš„Alpacaâ€ã€‚

[`https://github.com/tloen/alpaca-lora`](https://github.com/tloen/alpaca-lora)





# Chinese-LLaMA-Alpaca

Chinese LLaMA(ä¹Ÿç§°ä¸­æ–‡LLaMAï¼Œæœ‰7Bå’Œ13Bä¸¤ä¸ªç‰ˆæœ¬)ï¼Œç›¸å½“äºåœ¨åŸç‰ˆLLaMAçš„åŸºç¡€ä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨å¹¶ä½¿ç”¨äº†ä¸­æ–‡æ•°æ®è¿›è¡ŒäºŒæ¬¡é¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡äº†ä¸­æ–‡åŸºç¡€è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶ï¼Œåœ¨ä¸­æ–‡LLaMAçš„åŸºç¡€ä¸Šï¼Œä¸”ç”¨ä¸­æ–‡æŒ‡ä»¤æ•°æ®è¿›è¡ŒæŒ‡ä»¤ç²¾è°ƒå¾—Chinese-Alpaca(ä¹Ÿç§°ä¸­æ–‡Alpacaï¼ŒåŒæ ·ä¹Ÿæœ‰7Bå’Œ13Bä¸¤ä¸ªç‰ˆæœ¬)

[`https://github.com/ymcui/Chinese-LLaMA-Alpaca`](https://github.com/ymcui/Chinese-LLaMA-Alpaca)



## è¿è¡Œ

ä¸‹è½½ä¸­æ–‡æ¨¡å‹ã€‚

### æ‰‹åŠ¨æ¨¡å‹åˆå¹¶ä¸è½¬æ¢

[æ‰‹åŠ¨æ¨¡å‹åˆå¹¶ä¸è½¬æ¢ Â· ymcui/Chinese-LLaMA-Alpaca Wiki (github.com)](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/æ‰‹åŠ¨æ¨¡å‹åˆå¹¶ä¸è½¬æ¢)

1. å°†åŸç‰ˆLLaMAæ¨¡å‹è½¬æ¢ä¸ºHFæ ¼å¼

    è¯·ä½¿ç”¨[ğŸ¤—transformers](https://huggingface.co/docs/transformers/installation#install-from-source)æä¾›çš„è„šæœ¬[convert_llama_weights_to_hf.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py)ï¼Œå°†åŸç‰ˆLLaMAæ¨¡å‹è½¬æ¢ä¸ºHuggingFaceæ ¼å¼ã€‚

```Bash
python src/transformers/models/llama/convert_llama_weights_to_hf.py \
    --input_dir path_to_original_llama_root_dir \
    --model_size 7B \
    --output_dir path_to_original_llama_hf_dir
```
2. åˆå¹¶LoRAæƒé‡ï¼Œç”Ÿæˆæ¨¡å‹æƒé‡

    è¿™ä¸€æ­¥éª¤ä¼šå¯¹åŸç‰ˆLLaMAæ¨¡å‹ï¼ˆHFæ ¼å¼ï¼‰æ‰©å……ä¸­æ–‡è¯è¡¨ï¼Œåˆå¹¶LoRAæƒé‡å¹¶ç”Ÿæˆå…¨é‡æ¨¡å‹æƒé‡ã€‚æ­¤å¤„å¯ä»¥é€‰æ‹©è¾“å‡ºPyTorchç‰ˆæœ¬æƒé‡ï¼ˆ`.pth`æ–‡ä»¶ï¼‰æˆ–è€…è¾“å‡ºHuggingFaceç‰ˆæœ¬æƒé‡ï¼ˆ`.bin`æ–‡ä»¶ï¼‰ã€‚

    `.pth`æ–‡ä»¶å¯ç”¨äºï¼š

    - [ä½¿ç”¨llama.cppå·¥å…·è¿›è¡Œé‡åŒ–å’Œéƒ¨ç½²](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cppé‡åŒ–éƒ¨ç½²)

    `.bin`æ–‡ä»¶å¯ç”¨äºï¼š

    - [ä½¿ç”¨Transformersè¿›è¡Œæ¨ç†](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/ä½¿ç”¨Transformersæ¨ç†)
    - [ä½¿ç”¨text-generation-webuiæ­å»ºç•Œé¢](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/ä½¿ç”¨text-generation-webuiæ­å»ºç•Œé¢)

```Bash
python scripts/merge_llama_with_chinese_lora.py \
    --base_model path_to_original_llama_hf_dir \
    --lora_model path_to_chinese_llama_or_alpaca_lora \
    --output_type [pth|huggingface] \
    --output_dir path_to_output_dir 
```

### llama.cppé‡åŒ–éƒ¨ç½²

[llama.cppé‡åŒ–éƒ¨ç½² Â· ymcui/Chinese-LLaMA-Alpaca Wiki (github.com)](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cppé‡åŒ–éƒ¨ç½²)

1. ç”Ÿæˆé‡åŒ–ç‰ˆæœ¬çš„æ¨¡å‹

```Bash
# å°†ä¸Šè¿°.pthæ¨¡å‹æƒé‡è½¬æ¢ä¸ºggmlçš„FP16æ ¼å¼ï¼Œç”Ÿæˆæ–‡ä»¶è·¯å¾„ä¸ºzh-models/7B/ggml-model-f16.binã€‚
python convert.py zh-models/7B/
# è¿›ä¸€æ­¥å¯¹FP16æ¨¡å‹è¿›è¡Œ4-bité‡åŒ–ï¼Œç”Ÿæˆé‡åŒ–æ¨¡å‹æ–‡ä»¶è·¯å¾„ä¸ºzh-models/7B/ggml-model-q4_0.binã€‚
./quantize ./zh-models/7B/ggml-model-f16.bin ./zh-models/7B/ggml-model-q4_0.bin q4_0

```
2. åŠ è½½å¹¶å¯åŠ¨æ¨¡å‹

```Bash
./main -m zh-models/7B/ggml-model-q4_0.bin --color -f prompts/alpaca.txt -ins -c 2048 --temp 0.2 -n 256 --repeat_penalty 1.1

```

# BELLE

BELLEæ˜¯é“¾å®¶æ¨å‡ºçš„æ¨¡å‹ï¼Œä¹Ÿæ˜¯å¯¹LLaMAçš„å¾®è°ƒã€‚

[`https://github.com/LianjiaTech/BELLE`](https://github.com/LianjiaTech/BELLE)



# ChatGLM-6B

ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº [General Language Model (GLM)](https://github.com/THUDM/GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4 é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€ 6GB æ˜¾å­˜ï¼‰ã€‚ ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸­æ–‡é—®ç­”å’Œå¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯çš„åŠ æŒï¼Œ62 äº¿å‚æ•°çš„ ChatGLM-6B å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ã€‚

[`https://github.com/THUDM/ChatGLM-6B`](https://github.com/THUDM/ChatGLM-6B)



æ¨¡å‹åœ°å€ï¼š

`https://huggingface.co/THUDM/chatglm-6b`

## æ¨¡å‹æ¨ç†

1. cloneä»£ç 

```Bash
git clone https://github.com/THUDM/ChatGLM-6B.git
```
2. ä½¿ç”¨pipå®‰è£…ä¾èµ–ï¼š

```Bash
pip install -r requirements.txt
```
3. cloneæ¨¡å‹

```Bash
git clone https://huggingface.co/THUDM/chatglm-6b
```
4. è¿è¡Œç¨‹åºdemo

    æœ‰3ç§æ–¹å¼æ¥è¿è¡Œç¨‹åºï¼š

    1. å‘½ä»¤è¡Œæ–¹å¼

```Bash
python cli_demo.py
```
    2. ç½‘é¡µæ–¹å¼1

```Bash
python web_demo.py
```
    3. ç½‘é¡µæ–¹å¼2

```Bash
streamlit run web_demo2.py
```



# GLM-130B

GLM-130Bæ˜¯ä¸€ä¸ªå…·æœ‰1300äº¿ä¸ªå‚æ•°çš„å¤§æ¨¡å‹ã€‚æ”¯æŒä¸­è‹±åŒè¯­ï¼Œæ€§èƒ½å‡ºä¼—ã€‚

[`https://github.com/THUDM/GLM-130B`](https://github.com/THUDM/GLM-130B)



ä½†æ˜¯ç¡¬ä»¶è¦æ±‚è¾ƒé«˜ï¼Œæ²¡æœ‰éƒ¨ç½²æµ‹è¯•

|**Hardware**|**GPU Memory**|**Quantization**|**Weight Offload**|
|-|-|-|-|
|8 * A100|40 GB|No|No|
|8 * V100|32 GB|No|Yes (BMInf)|
|8 * V100|32 GB|INT8|No|
|8 * RTX 3090|24 GB|INT8|No|
|4 * RTX 3090|24 GB|INT4|No|
|8 * RTX 2080 Ti|11 GB|INT4|No|




# RWKV

RWKVæ ¹æ®æ–°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„ï¼Œä»¥å¾ªç¯ç¥ç»ç½‘ç»œRNNä¸ºåŸºç¡€é­”æ”¹è€Œæ¥ã€‚

[`https://github.com/BlinkDL/RWKV-LM`](https://github.com/BlinkDL/RWKV-LM)



æ¨¡å‹åœ°å€ï¼š

[`https://huggingface.co/BlinkDL/rwkv-4-raven`](https://huggingface.co/BlinkDL/rwkv-4-raven)



## è¿è¡Œ

```Bash
# å…‹éš†ChatRWKVé¡¹ç›®
git clone https://github.com/BlinkDL/ChatRWKV.git

# åˆ›å»ºè™šæ‹Ÿåœ°å€
conda create --name chatrwkv python=3.10
# æ¿€æ´»è™šæ‹Ÿåœ°å€
conda activate chatrwkv

# å®‰è£…ä¾èµ–
cd ChatRWKV/
pip install -r requirements.txt
pip install numpy
pip install torch

# åˆ›å»ºæ¨¡å‹æ–‡ä»¶å¤¹
mkdir models
# åœ¨modelsæ–‡ä»¶å¤¹ä¸­æ”¾ç½®RWKV-4-Raven-7B-v10x-Eng49%-Chn50%-Other1%-20230423-ctx4096.pthæ–‡ä»¶

# ä¿®æ”¹v2/chat.pyæ–‡ä»¶
args.strategy = 'cpu fp32'
CHAT_LANG = 'Chinese'
args.MODEL_NAME = '/home/ubuntu/ChatRWKV/models/RWKV-4-Raven-7B-v10x-Eng49%-Chn50%-Other1%-20230423-ctx4096.pth'

# æ‰§è¡Œchat.py
python v2/chat.py


```





> https://blog.csdn.net/v_JULY_v/article/details/129709105